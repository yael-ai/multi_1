from abc import ABC, abstractmethod
import numpy as np
from gym import spaces

from rl_agents.agents.common.abstract import AbstractStochasticAgent
from rl_agents.agents.common.exploration.abstract import exploration_factory
from rl_agents.agents.common.memory import ReplayMemory, Transition


class AbstractDQNAgent(AbstractStochasticAgent, ABC):
    def __init__(self, env, config=None):
        super(AbstractDQNAgent, self).__init__(config)
        self.env = env
        assert isinstance(env.action_space, spaces.Discrete) or isinstance(env.action_space, spaces.Tuple), \
            "Only compatible with Discrete action spaces."
        #yaeli
        #self.memory = ReplayMemory(self.config)
        #option 1:
        self.memory_1 = ReplayMemory(self.config)
        self.memory_2 = ReplayMemory(self.config)
        self.memory_3 = ReplayMemory(self.config)
        #option 2:
        self.memory = ReplayMemory(self.config)
        self.exploration_policy = exploration_factory(self.config["exploration"], self.env.action_space)
        self.training = True
        self.previous_state = None

    @classmethod
    def default_config(cls):
        return dict(model=dict(type="DuelingNetwork"),
                    optimizer=dict(type="ADAM",
                                   lr=5e-4,
                                   weight_decay=0,
                                   k=5),
                    loss_function="l2",
                    memory_capacity=50000,
                    batch_size=100,
                    gamma=0.99,
                    device="cuda:best",
                    exploration=dict(method="EpsilonGreedy"),
                    target_update=1,
                    double=True)
    #yaeli
    #def record(self, state, action, reward_all, next_state, done, info):
    def record(self, state, action, reward_all, reward_1, reward_2, reward_3, next_state, done, info):
        """
            Record a transition by performing a Deep Q-Network iteration

            - push the transition into memory
            - sample a minibatch
            - compute the bellman residual loss over the minibatch
            - perform one gradient descent step
            - slowly track the policy network with the target network
        :param state: a state
        :param action: an action
        :param reward: a reward
        :param next_state: a next state
        :param done: whether state is terminal
        """
        if not self.training:
            return
        if isinstance(state, tuple) and isinstance(action, tuple):  # Multi-agent setting
            [self.memory.push(agent_state, agent_action, reward, agent_next_state, done, info)
             for agent_state, agent_action, agent_next_state in zip(state, action, next_state)]
        else:  # Single-agent setting
           #yaeli
           #self.memory.push(state, action, reward_1, next_state, done, info)
           #OFRA+YAEL --> SUM REWARD LIKE UP
           self.memory.push(state, action, reward_all, next_state, done, info)
           #option 1:
           #all memorys are the same
           #self.memory_1.push_1(state, action, reward_1, next_state, done, info)
           #self.memory_2.push_2(state, action, reward_2, next_state, done, info)
           #self.memory_3.push_3(state, action, reward_3, next_state, done, info)
            #option 2:
           self.memory_1.push(state, action, reward_1, next_state, done, info)
           self.memory_2.push(state, action, reward_2, next_state, done, info)
           self.memory_3.push(state, action, reward_3, next_state, done, info)

        #yaeli
        batch = self.sample_minibatch()
        #option 1:
        batch_1 = self.sample_minibatch_1()
        batch_2 = self.sample_minibatch_2()
        batch_3 = self.sample_minibatch_3()


        #yaeli
        if batch:
            loss_1, _, _ = self.compute_bellman_residual(batch_1,1)
            loss_2, _, _ = self.compute_bellman_residual(batch_2,2)
            loss_3, _, _ = self.compute_bellman_residual(batch_3,3)
            #should the step optimizer be on the sum of the losses? should i caculate loss from thr batch of the reward_all?
            sum_loss = loss_1+loss_2+loss_3
            self.step_optimizer(sum_loss)
            self.update_target_network()
        #option 1:
        #if batch_1:
         #   loss_1, _, _ = self.compute_bellman_residual(batch_1)
        #if batch_2:
         #   loss_2, _, _ = self.compute_bellman_residual(batch_2)
        #if batch_3:
         #   loss_3, _, _ = self.compute_bellman_residual(batch_3)

        #if batch_1 or  batch_2 or  batch_3:
#        if batch_1 and batch_2 and batch_3:
 #           sum_loss = loss_1 + loss_2 + loss_3
  #          self.step_optimizer(sum_loss)
   #         self.update_target_network()
      #      self.step_optimizer(loss_3)
       #     self.update_target_network()

    def act(self, state, step_exploration_time=True):
        """
            Act according to the state-action value model and an exploration policy
        :param state: current state
        :param step_exploration_time: step the exploration schedule
        :return: an action
        """
        self.previous_state = state
        if step_exploration_time:
            self.exploration_policy.step_time()
        # Handle multi-agent observations
        # TODO: it would be more efficient to forward a batch of states
        if isinstance(state, tuple):
            return tuple(self.act(agent_state, step_exploration_time=False) for agent_state in state)

        # Single-agent setting
        # yaeli
        #OFRA+YAEL -> VALUES = SUM 1 ,2 ,3
        #for now - values = sum
        values = self.get_state_action_values(state)

        #option 2:
        #values_sum = values[0]
        self.exploration_policy.update(values[0])
        #self.exploration_policy.update(values_sum)
        return self.exploration_policy.sample()

    #yaeli
    def sample_minibatch(self):
        x = len(self.memory)
        if len(self.memory) < self.config["batch_size"]:
            return None
        transitions = self.memory.sample(self.config["batch_size"])
        return Transition(*zip(*transitions))
    #option 2:
    def sample_minibatch_1(self):
        x= len(self.memory_1)
        if len(self.memory_1) < self.config["batch_size"]:
            return None
        transitions = self.memory_1.sample(self.config["batch_size"])
        return Transition(*zip(*transitions))

    def sample_minibatch_2(self):
        x = len(self.memory_2)
        if len(self.memory_2) < self.config["batch_size"]:
            return None
        transitions = self.memory_2.sample(self.config["batch_size"])
        return Transition(*zip(*transitions))

    def sample_minibatch_3(self):
        if len(self.memory_3) < self.config["batch_size"]:
            return None
        transitions = self.memory_3.sample(self.config["batch_size"])
        return Transition(*zip(*transitions))
#end yaeli

    def update_target_network(self):
        self.steps += 1
        if self.steps % self.config["target_update"] == 0:
            self.target_net.load_state_dict(self.value_net.state_dict())

    @abstractmethod
    def compute_bellman_residual(self, batch, target_state_action_value=None):
        """
            Compute the Bellman Residual Loss over a batch
        :param batch: batch of transitions
        :param target_state_action_value: if provided, acts as a target (s,a)-value
                                          if not, it will be computed from batch and model (Double DQN target)
        :return: the loss over the batch, and the computed target
        """
        raise NotImplementedError

    @abstractmethod
    def get_batch_state_values(self, states):
        """
        Get the state values of several states
        :param states: [s1; ...; sN] an array of states
        :return: values, actions:
                 - [V1; ...; VN] the array of the state values for each state
                 - [a1*; ...; aN*] the array of corresponding optimal action indexes for each state
        """
        raise NotImplementedError

    @abstractmethod
    def get_batch_state_action_values(self, states):
        """
        Get the state-action values of several states
        :param states: [s1; ...; sN] an array of states
        :return: values:[[Q11, ..., Q1n]; ...] the array of all action values for each state
        """
        raise NotImplementedError

    def get_state_value(self, state):
        """
        :param state: s, an environment state
        :return: V, its state-value
        """
        values, actions = self.get_batch_state_values([state])
        return values[0], actions[0]

    def get_state_action_values(self, state):
        """
        :param state: s, an environment state
        :return: [Q(a1,s), ..., Q(an,s)] the array of its action-values for each actions
        """
        #yaeli TOP 1
        #return self.get_batch_state_action_values([state])[0]
        # option 2:
        return self.get_batch_state_action_values([state])

    def step_optimizer(self, loss):
        raise NotImplementedError

    def seed(self, seed=None):
        return self.exploration_policy.seed(seed)

    def reset(self):
        pass

    def set_writer(self, writer):
        super().set_writer(writer)
        try:
            self.exploration_policy.set_writer(writer)
        except AttributeError:
            pass

    def action_distribution(self, state):
        self.previous_state = state
        values = self.get_state_action_values(state)
        #yaeli
        #values_sum = values[0]
        self.exploration_policy.update(values[0])
        #self.exploration_policy.update(values_sum)
        return self.exploration_policy.get_distribution()

    def set_time(self, time):
        self.exploration_policy.set_time(time)

    def eval(self):
        self.training = False
        self.config['exploration']['method'] = "Greedy"
        self.exploration_policy = exploration_factory(self.config["exploration"], self.env.action_space)
